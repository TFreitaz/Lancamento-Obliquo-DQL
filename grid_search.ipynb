{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Otimização de parâmetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ztools.ntf import telegram # apagar - biblioteca particular\n",
    "\n",
    "from agent import Agent\n",
    "from environment import Environment\n",
    "from utils import plot_evolution\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_evolution(rewards: list, pack_size: int, game_name: str):\n",
    "    to_plot = []\n",
    "    mean = 0\n",
    "    for i, reward in enumerate(rewards):\n",
    "        if i % pack_size == 0:\n",
    "            to_plot.append(mean / pack_size)\n",
    "            mean = 0\n",
    "        mean += reward\n",
    "    plt.figure(figsize=(15, 4))\n",
    "    plt.plot(range(len(to_plot)), to_plot)\n",
    "    plt.axis((0, len(to_plot), -25, 50))\n",
    "    plt.xlabel(f\"Pack de {pack_size} episódios\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(f\"{game_name} DRL para lançamento oblíquo\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    # inicia parametros do objeto\n",
    "    def __init__(self, theta_disc: int, vel_disc: int, max_dist: float, target_len: float):\n",
    "\n",
    "        max_theta = 90  # ângulo máximo de lançamento\n",
    "        d_theta = max_theta / theta_disc  # discretização dos possíveis ângulos\n",
    "\n",
    "        max_vel = np.sqrt(max_dist * 9.81 / 2)  # máxima velocidade de lançamento possível\n",
    "        d_vel = max_vel / vel_disc  # discretização das possíveis velocidades\n",
    "\n",
    "        self.success_reward = 50  # recompensa para um acerto\n",
    "\n",
    "        self.max_dist = max_dist  # máxima distância do alvo\n",
    "        self.target_len = target_len  # comprimento do alvo (tolerância absoluta)\n",
    "        self.random_target_len = type(target_len) not in [float, int]  # definição de comprimento aleatório do alvo\n",
    "        self.theta_range = np.arange(0, max_theta + d_theta, d_theta)  # lista de ângulos discretos\n",
    "        self.vel_range = np.arange(0, max_vel + d_vel, d_vel)  # lista de velocidades discretas\n",
    "\n",
    "        self.actions = [\n",
    "            (theta, vel) for theta in self.theta_range for vel in self.vel_range\n",
    "        ]  # lista de ações de par velocidade-ângulo\n",
    "\n",
    "    def step(self, action: int, state: tuple):\n",
    "\n",
    "        theta, vel = self.actions[action]  # utiliza o index da ação selecionada para identificar ângulo e velocidade\n",
    "        d = 2 * vel ** 2 * np.sin(2 * theta) / 9.81  # calcula ponto final apos o lancamento com parâmetros selecionados\n",
    "\n",
    "        dist, target_len = state  # distância do alvo\n",
    "        err = abs(dist - d)  # distância entre o alvo e o ponto atingido (erro)\n",
    "\n",
    "        reward = -err  # torna o erro uma recompensa negativa\n",
    "\n",
    "        if err < target_len / 2:  # verifica se o ponto atingido esta dentro da tolerância do alvo\n",
    "            reward += self.success_reward  # insere recompensa por acerto\n",
    "\n",
    "        next_state = err  # define o ponto onde o projétil parou como próximo estado\n",
    "\n",
    "        return reward, next_state\n",
    "\n",
    "    def reset(self):\n",
    "        dist = np.random.random() * self.max_dist  # um ponto qualquer entre 0 e a distância máxima\n",
    "        target_len = np.random.random()*10+4 if self.random_target_len else self.target_len\n",
    "        return (dist, target_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    # inicia parâmetros do objeto\n",
    "    def __init__(self, sess: tf.Session, num_actions: int, num_states: int, gamma: float, min_experiences: int, max_experiences: int, batch_size: int):\n",
    "        self.sess = sess  # sessão do Tensorflow (funcional apenas para TensorFlow v.1)\n",
    "\n",
    "        self.num_actions = num_actions  # número de possíveis combinacoes para o par angulo-velocidade\n",
    "        self.min_experiences = min_experiences\n",
    "        self.max_experiences = max_experiences\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # placeholders (?)\n",
    "        self.states_ph = tf.placeholder(tf.float32, shape=(None, num_states))\n",
    "        self.targets_ph = tf.placeholder(tf.float32, shape=(None,))\n",
    "        self.actions_ph = tf.placeholder(tf.int32, shape=(None,))\n",
    "\n",
    "        # experiências\n",
    "        self.states: list = []\n",
    "        self.actions: list = []\n",
    "        self.rewards: list = []\n",
    "        self.next_states: list = []\n",
    "\n",
    "        self.gamma = gamma  # taxa de importância de eventos futuros\n",
    "\n",
    "        fc1 = tf.layers.dense(self.states_ph, 16, activation=tf.nn.relu)  # primeira camada da rede\n",
    "        fc2 = tf.layers.dense(fc1, 32, activation=tf.nn.relu)  # segunda camada da rede\n",
    "        self.Q_predicted = tf.layers.dense(fc2, self.num_actions, activation=None)  # camada de saída da rede\n",
    "        vet_Q_predicted = self.Q_predicted[tf.one_hot(self.actions_ph, self.num_actions, on_value=True, off_value=False)]  # (?)\n",
    "\n",
    "        loss = tf.losses.mean_squared_error(self.targets_ph, vet_Q_predicted)  # função de perda\n",
    "        self.optimizer = tf.train.AdamOptimizer(1e-3).minimize(loss)  # otimizador\n",
    "\n",
    "        self.init = tf.global_variables_initializer()  # inicialização das variáveis globais\n",
    "\n",
    "    # toma uma ação para um dado estado\n",
    "    def choose_action(self, state: int, eps: float):\n",
    "        if np.random.random() < eps:\n",
    "            return np.random.choice(range(self.num_actions))  # toma uma ação aleatória dentre as possibilidades\n",
    "        return np.argmax(self.predict_one(state))  # toma uma ação através da DQL\n",
    "\n",
    "    # insere experiências obtidas\n",
    "    def add_experience(self, state, action, reward, next_state):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.next_states.append(next_state)\n",
    "\n",
    "    def predict_one(self, state):\n",
    "        states = np.atleast_2d(state)\n",
    "        return self.sess.run(self.Q_predicted, feed_dict={self.states_ph: states})\n",
    "\n",
    "    def train(self):\n",
    "        if len(self.states) < self.min_experiences:\n",
    "            return\n",
    "        \n",
    "        min_index = m if (m:=len(self.states) - self.max_experiences) >= 0 else 0\n",
    "        \n",
    "        if self.batch_size:\n",
    "            avaliables = range(min_index, len(self.states))\n",
    "            indexes = np.random.choice(avaliables, size=self.batch_size, replace=False)\n",
    "        else:\n",
    "            indexes = range(min_index, len(self.states))\n",
    "        \n",
    "        selected_states = [self.states[i] for i in indexes]\n",
    "        selected_actions = [self.actions[i] for i in indexes]\n",
    "        selected_rewards = [self.rewards[i] for i in indexes]\n",
    "        \n",
    "        targets = selected_rewards  # valor incrementado pelas ações futuras. Não há ações futuras.\n",
    "\n",
    "        feed_dict = {self.states_ph: selected_states, self.actions_ph: selected_actions, self.targets_ph: targets}\n",
    "        self.sess.run(self.optimizer, feed_dict=feed_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'theta_disc': np.arange(80, 101, 10),\n",
    "    'vel_disc': np.arange(90, 111, 10),\n",
    "    # 'n_episodes': np.arange(1000, 8000, 2000),\n",
    "    # 'min_experiences': np.arange(0, 1000, 300),\n",
    "    # 'max_experiences': np.arange(4000, 15000, 3000),\n",
    "    # 'decay': np.arange(0.001, 0.012, 0.005),\n",
    "    # 'batch_size': np.arange(0, 1001, 1000),\n",
    "}\n",
    "\n",
    "n = 1\n",
    "for param in param_grid:\n",
    "    n *= len(param_grid[param])\n",
    "print('Número de combinações:', n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = []\n",
    "start = True\n",
    "for param in param_grid:\n",
    "    news = []\n",
    "    for value in param_grid[param]:\n",
    "        if start:\n",
    "            conf = {param: value}\n",
    "            news.append(conf)\n",
    "        else:\n",
    "            for conf in grid:\n",
    "                new_conf = conf.copy()\n",
    "                new_conf[param] = value\n",
    "                news.append(new_conf)\n",
    "    grid = news\n",
    "    start = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conf in grid:\n",
    "    env_settings = {\n",
    "        'theta_disc': int(conf['theta_disc']),  # número de pontos de discretização do ângulo de lancamento\n",
    "        'vel_disc': int(conf['vel_disc']),  # número de pontos de dicretização da velocidade de lancamento\n",
    "        'max_dist': 50,  # máxima distancia possivel para o alvo\n",
    "        'target_len': 'random'  # comprimento do alvo, isto e, tolerância absoluta para sucesso\n",
    "    }\n",
    "\n",
    "    n_states = 2\n",
    "\n",
    "    n_episodes = 4000  # número de episodios a serem executados\n",
    "    decay = 0.01  # decaimento da taxa de aleatoriedade\n",
    "\n",
    "    agent_settings = {\n",
    "        'num_states': 2,  # número de parâmetros de um estado = (distancia)\n",
    "        'gamma': 0,  # incremento por ações futuras\n",
    "        'min_experiences': 300,  # mínimo de experiências aleatórias\n",
    "        'max_experiences': 10000,  # máximo de experiências aleatórias\n",
    "        'batch_size': 0  # tamanho do pacote aleatório a ser treinado em cada episódio\n",
    "    }\n",
    "\n",
    "    total_reward = 0  # recompensa total\n",
    "    min_eps = 0.01  # mínima taxa de aleatoriedade\n",
    "    max_eps = 1  # máxima taxa de aleatoriedade\n",
    "    verbose = 0  # tipo de output visível após a execução\n",
    "    pack_size = 5  # números de episódios considerados em cada média no plot resultante\n",
    "\n",
    "    data = {\n",
    "        'agent_settings': agent_settings,\n",
    "        'env_settings': env_settings,\n",
    "        'training_settings': {\n",
    "            'n_states': n_states,\n",
    "            'n_episodes': n_episodes,\n",
    "            'min_eps': min_eps,\n",
    "            'max_eps': max_eps,\n",
    "            'decay': decay\n",
    "        }\n",
    "    }\n",
    "\n",
    "    env = Environment(**env_settings)  # ambiente configurado com parâmetros definidos\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        # saver = tf.train.Saver()\n",
    "        agent = Agent(sess, len(env.actions), **agent_settings)  # agente configurado com acoes definidas\n",
    "        sess.run(agent.init)  # inicalização da sessão para o agente (?)\n",
    "\n",
    "        all_episodes = range(n_episodes)  # index de episódios\n",
    "\n",
    "        if verbose == 0:\n",
    "            all_episodes = tqdm(all_episodes)  # barra de progressão por episódios\n",
    "\n",
    "        start_training = time.time()\n",
    "\n",
    "        for i in all_episodes:\n",
    "            eps = min_eps + (max_eps - min_eps) * np.exp(-decay * i)  # cálculo da taxa de aleatoriedade\n",
    "\n",
    "            state = env.reset()  # gera um novo ambiente = novo alvo\n",
    "            action = agent.choose_action(state, eps)  # toma uma ação dado o ambiente\n",
    "            reward, next_state = env.step(action, state)  # calcula os efeitos da ação tomada\n",
    "            agent.add_experience(state, action, reward, next_state)  # absorve a experiência obtida\n",
    "            agent.train()  # treino\n",
    "\n",
    "            if verbose == 1 and i % 100 == 0:\n",
    "                print(\"Reward:\", reward)\n",
    "\n",
    "        data['results'] = {\n",
    "            'training_rewards': agent.rewards,\n",
    "            'training_time': time.time() - start_training\n",
    "        }\n",
    "\n",
    "        print(\"\\nLast 100 episode rewards average:\", sum(agent.rewards[-100:]) / 100)\n",
    "        print(\"Total reward:\", sum(agent.rewards))\n",
    "        print('Total training time:', data['results']['training_time'], 's')\n",
    "\n",
    "        # saver.save(sess, 'data/test_session')\n",
    "\n",
    "        # plot_evolution(agent.rewards, pack_size, 'Treino')\n",
    "\n",
    "        print('\\nTesting.\\n')\n",
    "\n",
    "        n_episodes = 1000\n",
    "\n",
    "        rewards = []\n",
    "\n",
    "        for i in tqdm(range(n_episodes)):\n",
    "            state = env.reset()  # gera um novo ambiente = novo alvo\n",
    "            action = agent.choose_action(state, 0)  # toma uma ação dado o ambiente\n",
    "            reward, next_state = env.step(action, state)  # calcula os efeitos da ação tomada\n",
    "            rewards.append(reward)\n",
    "\n",
    "        data['results']['testing_rewards'] = rewards\n",
    "        data['date'] = datetime.today().isoformat()\n",
    "\n",
    "        if 'data' not in os.listdir():\n",
    "            os.mkdir('data')\n",
    "        filename = re.sub(r'[\\-\\:\\.]', '', data['date'])\n",
    "        with open(f\"data/{filename}.json\", 'w') as f:\n",
    "            try:\n",
    "                json.dump(data, f)\n",
    "            except Exception as e:\n",
    "                telegram('Erro no carregamento dos dados.\\n\\n' + str(e)) # apagar - biblioteca particular\n",
    "                raise e\n",
    "\n",
    "        # plot_evolution(rewards, pack_size, 'Teste')\n",
    "\n",
    "        print('\\nScore médio:', sum(rewards)/n_episodes)\n",
    "telegram('GridSearch completo.') # apagar - biblioteca particular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for file in os.listdir('data')[-8:]:\n",
    "    data.append(json.loads(open(f'data/{file}').read()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'vel_disc': [], 'theta_disc': [], 'reward': [], 'training_time': []}\n",
    "for d in data:\n",
    "    df['vel_disc'].append(d['env_settings']['vel_disc'])\n",
    "    df['theta_disc'].append(d['env_settings']['theta_disc'])\n",
    "    df['reward'].append(np.mean(d['results']['testing_rewards']))\n",
    "    df['training_time'].append(d['results']['training_time'])\n",
    "    \n",
    "result = pd.DataFrame(df)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(result['reward'], result['training_time'])\n",
    "plt.title('Desempenho das configurações testadas')\n",
    "plt.xlabel('Recompença média no teste')\n",
    "plt.ylabel('Tempo de treinamento')\n",
    "[plt.annotate(i, (result['reward'][i], result['training_time'][i]), xytext=(result['reward'][i]*1.005, result['training_time'][i]*1.005)) for i in result.index]\n",
    "plt.xlim([min(result['reward'].values)/1.02, max(result['reward'].values)*1.02])\n",
    "plt.ylim([min(result['training_time'].values)/1.02, max(result['training_time'].values)*1.02])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.iloc[3]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}